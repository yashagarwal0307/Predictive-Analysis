{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"## Importing necessary files","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading train data and cleaning it  ","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(r\"/kaggle/input/solar-panel-degradation/dataset/train.csv\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"humidity\"] = pd.to_numeric(df[\"humidity\"], errors='coerce')\ndf[\"wind_speed\"] = pd.to_numeric(df[\"wind_speed\"], errors='coerce')\ndf[\"pressure\"] = pd.to_numeric(df[\"pressure\"], errors='coerce')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode_value = df['installation_type'].mode()[0]\ndf['installation_type'].fillna(mode_value, inplace=True)\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['installation_type'] = le.fit_transform(df['installation_type'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_fill = ['irradiance','wind_speed',\"pressure\",\"humidity\", 'soiling_ratio', 'voltage', 'current',\"maintenance_count\",\"temperature\",\"cloud_coverage\",\"module_temperature\",\"panel_age\"]\n\nfor col in columns_to_fill:\n    df[col].fillna(df[col].mean(), inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['irradiance'] = df['irradiance'].where((df['irradiance'] >= 0) & (df['irradiance'] <= 1500), df['irradiance'].median)\ndf['humidity'] = df['humidity'].where((df['humidity'] >= 70) , df['humidity'].median)\ndf['soiling_ratio'] = df['soiling_ratio'].where((df['soiling_ratio'] <= 70) , df['soiling_ratio'].median())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Choosing relevant features, using MI,Correlation, VIF","metadata":{}},{"cell_type":"code","source":"df=df.drop([\"string_id\",\"error_code\",\"id\"], axis=1)\ncorrelation_matrix=df.corr()\nprint(\"correlation_matrix\", correlation_matrix)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n#from sklearn.feature_selection import mutual_info_classif  # for classification\nfrom sklearn.feature_selection import mutual_info_regression  # for regression\nfrom sklearn.preprocessing import LabelEncoder\n\n# Separate features and target\nX = df.drop('efficiency', axis=1)\ny = df['efficiency']\n\n# Encode categorical features if they exist\n#X = pd.get_dummies(X)\n\n# Compute mutual information\nmi_scores = mutual_info_regression(X, y)  # use mutual_info_regression for regression tasks\n\n# Create a DataFrame for better readability\nmi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores})\nmi_df = mi_df.sort_values(by='MI Score', ascending=False)\n\nprint(mi_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nimport pandas as pd\n\nX = add_constant(df[columns_to_fill])\nvif = pd.DataFrame()\nvif[\"feature\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df.drop([\"installation_type\",\"temperature\",\"pressure\",\"maintenance_count\",\"module_temperature\",\"wind_speed\",\"cloud_coverage\"],axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training different ML models\n<The best one being the ensemble Technique>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\n# Step 3: Encode categorical features (if any)\ndf = pd.get_dummies(df)\n\n# Step 4: Separate features and target\nZ = df.drop(\"efficiency\", axis=1)  # replace with your target\nr = df[\"efficiency\"]\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nZ_scaled = scaler.fit_transform(Z)\n\n# Step 5: Train-test split\nX_train, X_test, y_train, y_test = train_test_split(Z_scaled, r, test_size=0.3, random_state=0)\n\n# Step 6: Create and train the XGBoost model\nmodel = xgb.XGBRegressor()  # Use XGBRegressor() for regression\nmodel.fit(X_train, y_train)\n\n# Step 7: Predictions\ny_pred = model.predict(X_test)\n# For regression (if using XGBRegressor)\nprint(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=4,n_estimators=1200)  # or RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Step 7: Make predictions\ny_pred = model.predict(X_test)\nprint(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.svm import SVR  # Support Vector Regressor\nmodel1 = SVR(kernel='rbf',gamma='auto',C=0.15)\nmodel1.fit(X_train, y_train)\ny_pred = model1.predict(X_test)\nprint(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n# gradient bossting regressor.\nfrom sklearn.ensemble import GradientBoostingRegressor\nest = GradientBoostingRegressor(\n    n_estimators=1200, learning_rate=0.01, max_depth=3, random_state=50,\n    loss='squared_error'\n)\nest = est.fit(X_train, y_train)\ny_pred = est.predict(X_test)\nScore = 100*(1-np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"score:\", Score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmodel = CatBoostRegressor(verbose=0, iterations=1000, learning_rate=0.01, depth=7, random_state=200)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\nprint(\"R² Score:\", r2_score(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR \nfrom sklearn.ensemble import RandomForestRegressor\n\n\nreg1 = GradientBoostingRegressor(random_state=10,n_estimators=1200, learning_rate=0.01, max_depth=3, loss='squared_error')\nreg2 = RandomForestRegressor(random_state=1,n_estimators=1200)\nreg3 = CatBoostRegressor(verbose=0, iterations=1000, learning_rate=0.01, depth=7, random_state=2)\nreg4 = SVR(kernel='rbf',gamma='auto',C=0.15)\n\n\nereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2),('cb',reg3),('sv',reg4)])\nereg = ereg.fit(X_train, y_train)\ny_pred = ereg.predict(X_test)\nprint(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\nScore = 100*(1-np.sqrt(mean_squared_error(y_test,y_pred)))\nprint(\"score:\", Score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ELMRegressor:\n    def __init__(self, n_hidden=100, activation='sigmoid',random_state=None):\n        self.n_hidden = n_hidden\n        self.activation = activation\n\n    def _activate(self, X):\n        if self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-X))\n        elif self.activation == 'tanh':\n            return np.tanh(X)\n        elif self.activation == 'relu':\n            return np.maximum(0, X)\n        else:\n            raise ValueError(\"Unsupported activation\")\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Random weights and biases\n        self.input_weights = np.random.randn(n_features, self.n_hidden)\n        self.biases = np.random.randn(self.n_hidden)\n\n        # Hidden layer output\n        H = self._activate(np.dot(X, self.input_weights) + self.biases)\n\n        # Moore-Penrose pseudoinverse to solve output weights\n        self.output_weights = np.dot(np.linalg.pinv(H), y)\n\n    def predict(self, X):\n        H = self._activate(np.dot(X, self.input_weights) + self.biases)\n        return np.dot(H, self.output_weights)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elm = ELMRegressor(n_hidden=500, activation='sigmoid')\nelm.fit(X_train, y_train)\n\n# Predict\ny_pred_scaled = elm.predict(X_test)\n# Evaluation\nprint(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\nprint(\"R² Score:\", r2_score(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading test data for final prediction","metadata":{}},{"cell_type":"code","source":"df_test=pd.read_csv(\"/kaggle/input/solar-panel-degradation/dataset/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"humidity\"] = pd.to_numeric(df_test[\"humidity\"], errors='coerce')\ndf_test[\"wind_speed\"] = pd.to_numeric(df_test[\"wind_speed\"], errors='coerce')\ndf_test[\"pressure\"] = pd.to_numeric(df_test[\"pressure\"], errors='coerce')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test=df_test.drop([\"string_id\",'cloud_coverage',\"module_temperature\",\"maintenance_count\",\"wind_speed\",\"error_code\",\"id\",\"temperature\",\"pressure\",\"installation_type\"], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_fill = ['irradiance',\"humidity\", 'soiling_ratio', 'voltage', 'current',\"panel_age\"]\n\nfor col in columns_to_fill:\n    df_test[col].fillna(df_test[col].median(), inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['irradiance'] = df_test['irradiance'].where((df_test['irradiance'] >= 0) & (df_test['irradiance'] <= 1500), df_test['irradiance'].median())\ndf_test['humidity'] = df_test['humidity'].where((df_test['humidity'] >= 70) , df_test['humidity'].median())\ndf['soiling_ratio'] = df['soiling_ratio'].where((df['soiling_ratio'] <= 70) , df_test['soiling_ratio'].median())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_1=pd.read_csv(\"/kaggle/input/solar-panel-degradation/dataset/test.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_scaled1 = scaler.fit_transform(df_test)\ny_test_pred1 =ereg.predict(X_scaled1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[\"efficiency\"]=y_test_pred1\ndf_test[\"id\"]=df_test_1[\"id\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z=df_test[[\"id\",\"efficiency\"]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z.to_csv(\"submission1.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" Note:-All the models were equally good, but ensemble using voting regressor showed best results","metadata":{}}]}